{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chebyshev polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is method to estimate the exponential of a matrix based on a polynomial approximation exponential function for an argument $|x|<1$. The idea is that, given a function $f(\\omega)$ which is defined in the complex domain $\\omega \\in[-i,i]$, it can be very well approximated using the Chebyshev polynomials\n",
    "\\begin{eqnarray}\n",
    "T_0 &=&1,\\\\\n",
    "T_1 &=&x,\\\\\n",
    "T_n &=& 2x T_k(x) - T_{k-1}(x)\n",
    "\\end{eqnarray}\n",
    "using the orthonormal expansion\n",
    "$$f(\\omega) = \\sum_k a_k T_k(\\omega),$$\n",
    "where $a_k$ are the projection of the function on that domain with a particular measure\n",
    "$$a_k = -i\\int_{-i}^i (1-|\\omega|^2)^{-1/2}f(\\omega) T_k(\\omega)\\mathrm{d}\\omega.$$\n",
    "We use this expansion to approximate the evolution operator\n",
    "$$\\exp(-i \\Delta t H) = e^{-i r_+}\\exp(r_- B),$$\n",
    "where $r_\\pm = \\Delta t(\\lambda_{max}\\pm\\lambda_{min})/2$ is built from the maximum and minimum eigenvalues $\\lambda_{max}$ and $\\lambda_{min}$ of $H$, to construct an operator\n",
    "$$B = \\frac{-i}{r_-}(\\Delta t H - r_+),$$\n",
    "whose spectrum lays in $[-i,i]$. Using this, we have\n",
    "$$\\exp(-i \\Delta t H) = e^{-ir_+}\\left[J_0(r_-) + 2\\sum_k J_k(r_-)T_k(B)\\right],$$\n",
    "where $J_k(x)$ are the Bessel functions of the first kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: seeq/chebyshev.py\n",
    "\n",
    "import numpy\n",
    "import scipy.sparse.linalg\n",
    "from scipy.special import jn\n",
    "import scipy.linalg as la\n",
    "import scipy.sparse.linalg as sla\n",
    "import warnings\n",
    "\n",
    "class AccuracyWarning(Warning):\n",
    "    pass\n",
    "\n",
    "class ChebyshevExpm:\n",
    "    \"\"\"\n",
    "    This object is created to compute $exp(-i*H*dt)*v$ using\n",
    "    the Chebyshev series to approximate the exponential. The\n",
    "    object does some preconditioning or scaling of the operator\n",
    "    based on the range of eigenvalues and 'factor', transforming\n",
    "    it to $exp(-i*A*R)*v$ where B has eigenvalues in [-1,1] and\n",
    "    R is a real number.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    H         -- A matrix or a LinearOperator() with shape (d,d)\n",
    "    d         -- matrix size, only needs to be provided when H is a function\n",
    "    bandwidth -- An upper bound on the spectral bandwidth of A\n",
    "    \"\"\"\n",
    "    def __init__(self, H, d=None, bandwidth=None):\n",
    "        #\n",
    "        # H can be a function, a matrix, a sparse matrix, etc. We construct\n",
    "        # a linear operator in all cases, which is a general abstraction that\n",
    "        # numpy can work with and allows multiplication times a vector '@'\n",
    "        #\n",
    "        # The method demands that the eigenvalues be in the range [-1,1]\n",
    "        # We need analyze the operator to estimate a range of eigenvalues\n",
    "        # and rescale the operator \"A*factor\" to a smaller range.\n",
    "        #\n",
    "        if callable(H) and not isinstance(H, sla.LinearOperator):\n",
    "            H = scipy.sparse.linalg.LinearOperator((d,d),matvec=H)\n",
    "        self.H = H\n",
    "        #\n",
    "        # Estimate the spectral range of H, computing the smallest and\n",
    "        # largest eigenvalues. ARPACK in standard Python is not good at\n",
    "        # computing small values. We thus use a trick of computing the\n",
    "        # largest magnitude X and assume the spectrum is in [-X,X]\n",
    "        #\n",
    "        if bandwidth is None:\n",
    "            λmax = sla.eigs(H, k=1, which='LM', return_eigenvectors=0)[0]\n",
    "            Hnorm = abs(λmax)\n",
    "            bandwidth = 2*Hnorm\n",
    "        if numpy.isscalar(bandwidth):\n",
    "            self.height = bandwidth/2.0\n",
    "            self.center = 0.0\n",
    "        else:\n",
    "            Emin, Emax = bandwidth\n",
    "            self.height = 0.5 * (Emax - Emin)\n",
    "            self.center = 0.5 * (Emax + Emin)\n",
    "\n",
    "    @staticmethod\n",
    "    def weights(order, rm):\n",
    "        ndx = numpy.arange(0,order)\n",
    "        return jn(ndx, rm) * ((-1j)**ndx)\n",
    "                    \n",
    "    def apply(self, v, dt=1.0, order=None, maxorder=None, tol=1e-14):\n",
    "        \"\"\"Apply the Chebyshev approximation of the exponential exp(-1i*dt*A)\n",
    "        onto the vector or matrix `v`.        \n",
    "        Parameters\n",
    "        ----------\n",
    "        v     -- A vector or a matrix\n",
    "        order -- the order of the Chebyshev approximation\n",
    "        dt    -- time interval in the exponential above\n",
    "        tol   -- relative tolerance for deciding when to stop the\n",
    "                 Chebyshev expansion\n",
    "        \"\"\"\n",
    "        rp = dt * self.center\n",
    "        rm = dt * self.height\n",
    "        if order is None:\n",
    "            order = max(100, 2*int(rm))\n",
    "        \n",
    "        # Apply a version of A that is shifted and rescaled \n",
    "        def Btimes(phi):\n",
    "            #\n",
    "            # There's something stupid about LinearOperators that always return\n",
    "            # matrices when applied to arrays. This screws our use of vdot()\n",
    "            # and other operations below\n",
    "            return numpy.asarray((self.H @ phi) * (dt/rm) - phi * (rp/rm))\n",
    "    \n",
    "        # Bessel coefficients ak, plus imaginary parts from chebyshev polynomials    \n",
    "        ak = self.weights(order, rm)\n",
    "    \n",
    "        # Zeroth and first order\n",
    "        phi0 = v\n",
    "        phi1 = Btimes(phi0)\n",
    "        cheb = ak[0] * phi0 + 2 * ak[1] * phi1\n",
    "    \n",
    "        # We can define an absolute tolerance if we assume unitary evolution\n",
    "        # and always a nonzero relative tolerance 'tol'\n",
    "        # Note the 'vector' 'v' is actually a matrix of vectors with columns\n",
    "        # corresponding to different states. We want to compute the total\n",
    "        # norm of the vectors in a speedy way.\n",
    "        atol2 = numpy.abs(tol**2 * numpy.vdot(v, v))\n",
    "        \n",
    "        # Higher orders\n",
    "        for jj in range(2,order):\n",
    "            phi2 = 2 * Btimes(phi1) - phi0\n",
    "            tmp = 2 * ak[jj] * phi2\n",
    "            cheb += tmp\n",
    "            if abs(numpy.vdot(tmp,tmp)) < atol2:\n",
    "                break\n",
    "            phi0 = phi1\n",
    "            phi1 = phi2\n",
    "        else:\n",
    "            warnings.warn(f'Desired precision {tol} not reached with {order} steps. '\n",
    "                          f'Error is {numpy.linalg.norm(tmp)/numpy.linalg.norm(cheb)}',\n",
    "                          AccuracyWarning)\n",
    "\n",
    "        return cheb * numpy.exp(-1j*rp)\n",
    "    \n",
    "def expm(A, v, d=None, bandwidth=None, **kwargs):\n",
    "    \"\"\"Apply the Chebyshev approximation of the exponential exp(1i*dt*A)\n",
    "    onto the vector or matrix `v`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A         -- A matrix or a LinearOperator() with shape (d,d), or\n",
    "                 a linear function that acts on vectors.\n",
    "    v         -- A vector or a matrix with shapes (d,) or (d,M)\n",
    "    d         -- Dimension of matrix A. Only required when A is\n",
    "                 a function or callable object\n",
    "    bandwidth -- An upper bound on the spectral bandwidth of A or\n",
    "                 a pair (Emin, Emax) of extreme eigenvalues\n",
    "    order     -- the order of the Chebyshev approximation\n",
    "    dt        -- time interval in the exponential above\n",
    "    tol       -- relative tolerance for deciding when to stop the\n",
    "                 Chebyshev expansion\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    newv      -- A vector or a matrix approximating expm(-1j*dt*A) @ v\n",
    "    \"\"\"\n",
    "    aux = ChebyshevExpm(A, d=d, bandwidth=bandwidth)\n",
    "    return aux.apply(v, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chebyshevExpm = expm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krylov methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to compute a  function $f(A)$ of some linear operator $A$, acting on a vector $v_0$. Krylov methods building an approximation to $f(A)*v_0$ in the Krylov space of vectors given by\n",
    "$$K = \\text{lin}\\{ v, A v, A^2 v, \\ldots \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This space of vectors can be constructed from an orthogonal set of vectors\n",
    "$$v_n \\propto A v_{n-1} - \\sum_{m\\leq n} (v_{m}^\\dagger A v_{n-1}) v_{m}$$\n",
    "where $v_1=v$ is the initial vector mentioned above and\n",
    "$$K = \\text{lin}\\{v_1,v_2\\ldots\\}$$\n",
    "There are two alternative approaches, depending on whether our operator is Hermitian or not. We follow the work by Y. Saad [(SIAM J. Numer. Anala. 29(1), 209-228.)](https://doi.org/10.1137/0729014) ([PDF here](https://www.jstor.org/stable/pdf/2158085.pdf)) for the definition of the algorithms and the error analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krylov exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is not Hermitian, we need construct an orthonormal basis of vectors, $V_m = [v_1,v_2,\\ldots,v_m]$ to describe the Krylov subspace $K_m$ where we will implement the approximation of the exponential. The basis is constructed recursively, with the following algorithm\n",
    "\n",
    "1. Initialize. Compute $v_1=v/\\Vert{v}\\Vert_2$\n",
    "2. Iterate. Do $j=1,2,\\ldots,m$\n",
    "   1. Compute $w := A v_j$\n",
    "   2. Do $i=1,2,\\ldots,j$\n",
    "       1. Compute $h_{i,j} = (w, v_i)$\n",
    "       2. Compute $w := w - h_{ij} v_i$\n",
    "   3. Compute $h_{j+1,j} := \\Vert{w}\\Vert_2, \\; v_{j+1}:=w/h_{j+1,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via this process, we are computing a Hessemberg matrix $(H_m)_{ij}=h_{ij}$ with the relation\n",
    "$$A V_m = V_m H_m + h_{m+1,m} v_{m+1} e^T_m,$$\n",
    "where $e_m$ is the unit vector where only the $m-$th component is nonzero.\n",
    "In other words, $H_m$ is an almost exact representation of $A$ in the new basis and the second term is the error. This also allows to approximate\n",
    "$$e^{\\tau A} v \\simeq \\Vert{v}\\Vert_2 V_m e^{\\tau H_m} e_1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know how many vectors we need to compute so as to make the error $\\varepsilon_m$ small enough. There exist various a-priori estimates that depend on the spectral radius of $A$, the factor $\\tau$ and the order of approximation $m$. When $A$ is skew-symmetric, a good upper bound is\n",
    "$$\\varepsilon_m \\leq 12 e^{-(\\rho\\tau)^2/m}\\left(\\frac{e\\rho\\tau}{m}\\right)^m < \\frac{\\Vert A\\Vert^m}{m!}$$\n",
    "provided $m > 2\\rho\\tau$. Another upper bound is\n",
    "$$\\varepsilon_m < \\frac{\\Vert A\\Vert^m}{m!},\\quad m > \\Vert A\\Vert$$\n",
    "Neither of them is too tight, but it could serve us to guess a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our algorithm below, we do not rely so much on a-priori bounds. Instead we use a guess for the order of approximation and then verify it with an a-posteriori bound. This later bound uses the information on the exponential of the matrix $H$, as well as the latest $\\beta$ that has been computed\n",
    "$$\\Vert\\varepsilon_m\\Vert \\leq \\Vert{v}\\Vert_2 h_{m+1,m} |e^T_m e^{H_m} e_1|$$\n",
    "We use this bound to repeatedly increase the number of vectors until convergence. Note, however, that there is a limit in the precission imposed by the numerical errors in the computer. This has to be taken in to account to stop the recursion when we hit that wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: seeq/arnoldi.py\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.linalg as la\n",
    "import scipy.sparse.linalg as sla\n",
    "import warnings\n",
    "\n",
    "class AccuracyWarning(Warning):\n",
    "    pass\n",
    "\n",
    "class ArnoldiExpm:\n",
    "    def __init__(self, H, d=0):\n",
    "        #\n",
    "        # H can be a function, a matrix, a sparse matrix, etc. We construct\n",
    "        # a linear operator in all cases, which is a general abstraction that\n",
    "        # numpy can work with and allows multiplication times a vector '@'\n",
    "        #\n",
    "        if callable(H) and not isinstance(H, sla.LinearOperator):\n",
    "            H = sla.LinearOperator((d,d),matvec=H)\n",
    "        self.H = H            \n",
    "    \n",
    "    def estimateOrder(self, dt=1.0):\n",
    "        #\n",
    "        # Estimate the order of the Lanczos expansion\n",
    "        #\n",
    "        self.Hnorm = abs(sla.eigs(self.H, k=1, which='LM', return_eigenvectors=0)[0])\n",
    "        return max(int(3*self.Hnorm*dt+1),4)\n",
    "\n",
    "    def apply(self, v, dt=1.0, order=None, adaptive=False, dtmin=None, tol=1e-12, warning=True):\n",
    "        \"\"\"Apply the Arnodi approximation of the exponential exp(-1i*dt*A)\n",
    "        onto the vector or array `v`.        \n",
    "        Parameters\n",
    "        ----------\n",
    "        v        -- A vector or a matrix\n",
    "        order    -- Maximum number of Arnoldi vectors\n",
    "        dt       -- time interval in the exponential above (can be complex)\n",
    "        tol      -- relative tolerance for deciding when to stop the expansion\n",
    "        adaptive -- Reduce time step if errors exceed tolerance\n",
    "        dtmin    -- Minimum time interval allowed when adaptive = True\n",
    "        warning  -- Emit warning when errors exceed tolerance\n",
    "        \"\"\"\n",
    "        nmax = 12 if order is None else order\n",
    "        if dtmin is None:\n",
    "            dtmin = dt / 128\n",
    "        #\n",
    "        # While 'v' could be any tensor, we have to rewrite it in\n",
    "        # matrix form because if self.H is a LinearOperator or a\n",
    "        # sparse matrix, it only works with objects that have <= 2\n",
    "        # indices\n",
    "        v = numpy.asarray(v)\n",
    "        shape = v.shape\n",
    "        if v.ndim > 1:\n",
    "            shape = (shape[0],v.size // shape[0])\n",
    "        start = 0\n",
    "        t = dt\n",
    "        δt = dt\n",
    "        while t > δt/2:\n",
    "            if start == 0:\n",
    "                β = numpy.linalg.norm(v)\n",
    "                vn = v.flatten() / β\n",
    "                V = []\n",
    "                H = numpy.zeros((nmax+1,nmax+1),dtype=numpy.complex128)\n",
    "            else:\n",
    "                aux = np.zeros((nmax+1,nmax+1), dtype=H.dtype)\n",
    "                aux[numpy.ix_(range(H.shape[0]),range(H.shape[1]))] = H\n",
    "                H = aux\n",
    "            for j in range(start,nmax):\n",
    "                V.append(vn)\n",
    "                w = (self.H @ vn.reshape(shape)).flatten()\n",
    "                for (i,Vi) in enumerate(V):\n",
    "                    H[i,j] = hij = numpy.vdot(Vi, w)\n",
    "                    w -= hij * Vi\n",
    "                H[j+1,j] = hlast = numpy.linalg.norm(w)\n",
    "                if hlast < 1e-16:\n",
    "                    # Stop if our vectors become too small\n",
    "                    break\n",
    "                w /= hlast\n",
    "                vn = w\n",
    "            #\n",
    "            # We diagonalize the banded matrix formed by α and β and\n",
    "            # use that to compute the exponential of the effective\n",
    "            # truncated matrix. This also allows us to estimate the error\n",
    "            # due to the Lanczos method.\n",
    "            #\n",
    "            # Corrected Arnoldi method\n",
    "            H[j+1,j+1] = 1.0\n",
    "            e1 = numpy.zeros(j+2)\n",
    "            e1[0] = 1.0\n",
    "            y = scipy.sparse.linalg.expm_multiply((-1j*δt)*H, e1)\n",
    "            err, y = abs(hlast * y[-1]), y[:-1]\n",
    "            #\n",
    "            if err <= tol:\n",
    "                # Error is small, we keep approximation, advance time\n",
    "                pass\n",
    "            elif (order is not None) and adaptive:\n",
    "                # Error is big, try reducing time step\n",
    "                while err > tol and δt > dtmin*dt:\n",
    "                    δt /= 2\n",
    "                    y = scipy.sparse.linalg.expm_multiply((-1j*δt)*H, e1)\n",
    "                    err, y = abs(hlast * y[-1]), y[:-1]\n",
    "                adaptive = False\n",
    "            elif (order is None) and (nmax < v.size):\n",
    "                # Try mitigating error by increasing number of Arnoldi\n",
    "                # vectors, if feasible.\n",
    "                start = nmax\n",
    "                nmax = min(int(1.5*nmax+1), v.size)\n",
    "                continue\n",
    "            else:\n",
    "                # We cannot reduce time, and cannot enlarge the number\n",
    "                # of vectors, emit a warning if we did not so before\n",
    "                if warning:\n",
    "                    warnings.warn(f'Arnoldi failed to converge at {j+1} '\n",
    "                                  f'iterations with error {err}',\n",
    "                                  AccuracyWarning)\n",
    "                warning = False\n",
    "            #\n",
    "            # Compute the new vector and (possibly) start again at an\n",
    "            # increased time\n",
    "            v = sum(Vi * (β * yi) for Vi, yi in zip(V, y)).reshape(v.shape)\n",
    "            start = 0\n",
    "            t -= δt\n",
    "        return v\n",
    "    \n",
    "def expm(A, v, **kwargs):\n",
    "    \"\"\"Apply the Arnoldi approximation of the exponential exp(-1i*dt*A)\n",
    "    onto the vector or matrix `v`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A         -- A matrix or a LinearOperator() with shape (d,d), or\n",
    "                 a linear function that acts on vectors.\n",
    "    v         -- A vector or a matrix with shapes (d,) or (d,M)\n",
    "    d         -- Dimension of matrix A. Only required when A is\n",
    "                 a function or callable object\n",
    "    order     -- maximum order of the Arnoldi approximation\n",
    "    dt        -- time interval in the exponential above (can be complex)\n",
    "    tol       -- relative tolerance for deciding when to stop\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    newv      -- A vector or a matrix approximating expm(1j*dt*A) @ v\n",
    "    \"\"\"\n",
    "    aux = ArnoldiExpm(A, d=v.size)\n",
    "    return aux.apply(v, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arnoldiExpm = expm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lanczos exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is Hermitian, two things happen. The first one is that the projection of $A$ onto the basis $\\{v_n\\}$ is very simple\n",
    "$$H_{mm}:=(v_{m}^\\dagger A v_{n})={\\begin{pmatrix}\\alpha _{1}&\\beta _{2}&&&&0\\\\\\beta _{2}&\\alpha _{2}&\\beta _{3}&&&\\\\&\\beta _{3}&\\alpha _{3}&\\ddots &&\\\\&&\\ddots &\\ddots &\\beta _{m-1}&\\\\&&&\\beta _{m-1}&\\alpha _{m-1}&\\beta _{m}\\\\0&&&&\\beta _{m}&\\alpha _{m}\\\\\\end{pmatrix}}$$\n",
    "The second one is that the iterative construction of the orthonormal basis is very simple and involves only one product of matrix times a vector, and two additions\n",
    "$$v_{n+1} = \\frac{1}{\\beta_{n+1}}(A v_n - \\alpha_n v_n - \\beta_n v_{n-1})$$\n",
    "$$\\alpha_n = (v_n^\\dagger A v_n)$$\n",
    "$$\\beta_n = (v_{n-1}^\\dagger A v_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our algorithm below, we do not rely so much on a-priori bounds. Instead we use a guess for the order of approximation and then verify it with an a-posteriori bound. This later bound uses the information on the exponential of the matrix $T$, as well as the latest $\\beta$ that has been computed\n",
    "$$\\Vert\\varepsilon_m\\Vert \\leq \\left[exp(-\\tau H)\\right]_{m,1}\\times |\\beta_{m}|$$\n",
    "We use this bound to repeatedly increase the number of vectors until convergence. Note, however, that there is a limit in the precission imposed by the numerical errors in the computer. This has to be taken in to account to stop the recursion when we hit that wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: seeq/lanczos.py\n",
    "\n",
    "import numpy\n",
    "import scipy\n",
    "import scipy.linalg as la\n",
    "import scipy.sparse.linalg as sla\n",
    "import warnings\n",
    "\n",
    "class AccuracyWarning(Warning):\n",
    "    pass\n",
    "\n",
    "class LanczosExpm:\n",
    "    def __init__(self, H, d=0):\n",
    "        #\n",
    "        # H can be a function, a matrix, a sparse matrix, etc. We construct\n",
    "        # a linear operator in all cases, which is a general abstraction that\n",
    "        # numpy can work with and allows multiplication times a vector '@'\n",
    "        #\n",
    "        if callable(H) and not isinstance(H, sla.LinearOperator):\n",
    "            H = sla.LinearOperator((d,d),matvec=H)\n",
    "        self.H = H\n",
    "\n",
    "    def apply(self, v, dt=1.0, order=None, tol=1e-14):\n",
    "        \"\"\"Apply the Lanczos approximation of the exponential exp(-1i*dt*A)\n",
    "        onto the vector `v`.        \n",
    "        Parameters\n",
    "        ----------\n",
    "        v     -- A vector\n",
    "        order -- Maximum number of Arnoldi vectors\n",
    "        dt    -- time interval in the exponential above (can be complex)\n",
    "        tol   -- relative tolerance for deciding when to stop the\n",
    "                 Arnoldi expansion\n",
    "        \"\"\"\n",
    "        # This function has two ways to operate: if we provide an order,\n",
    "        # it applies the Lanczos expansion up to that basis size; otherwise\n",
    "        # it estimates the number of vectors based on the norm of the matrix\n",
    "        # that we will exponentiate (which was estimated before in __init__)\n",
    "        nmax = 10 if order is None else order\n",
    "        if nmax > v.size:\n",
    "            nmax = v.size\n",
    "        #\n",
    "        # Construct a projected version of the matrix 'H' on the\n",
    "        # Krylov subspace generated around vector 'v'\n",
    "        #\n",
    "        v = numpy.array(v)\n",
    "        vnrm = la.norm(v)\n",
    "        vn = v / vnrm\n",
    "        vnm1 = numpy.zeros(v.shape)\n",
    "        α = []\n",
    "        β = [0.0]\n",
    "        start = 0\n",
    "        lasterr = vnrm * 1e10\n",
    "        while True:\n",
    "            #\n",
    "            # Iteratively extend the Krylov basis using the lanczos\n",
    "            # recursion without restart or reorthogonalization.\n",
    "            #\n",
    "            for n in range(start, nmax):\n",
    "                w = numpy.asarray(self.H @ vn)\n",
    "                α.append(numpy.vdot(vn, w))\n",
    "                w = w - α[n] * vn - β[n] * vnm1\n",
    "                vnm1 = vn\n",
    "                aux = la.norm(w)\n",
    "                β.append(aux)\n",
    "                if aux < 1e-20:\n",
    "                    break\n",
    "                vn = w / aux\n",
    "            #\n",
    "            # We diagonalize the banded matrix formed by α and β and\n",
    "            # use that to compute the exponential of the effective\n",
    "            # truncated matrix. This also allows us to estimate the error\n",
    "            # due to the Lanczos method.\n",
    "            #\n",
    "            w, u = scipy.linalg.eig_banded(numpy.array([β[:-1],α]),\n",
    "                                           overwrite_a_band=True)\n",
    "            fHt = u @ (numpy.exp(-1j*dt*w) * u[0,:].conj())\n",
    "            err = abs(fHt[n]*β[n+1])\n",
    "            if err < tol:\n",
    "                break\n",
    "            if lasterr < err or nmax == v.size or (order is not None):\n",
    "                warnings.warn(f'Lanczos failed to converge at {len(α)} iterations with error {err}',\n",
    "                              AccuracyWarning)\n",
    "                lasterr = err\n",
    "                break\n",
    "            start = nmax\n",
    "            nmax = min(int(1.5*nmax+1), v.size)\n",
    "        #\n",
    "        # Given the approximation of the exponential, recompute the\n",
    "        # Lanczos basis \n",
    "        #\n",
    "        vnm1 = vn = v\n",
    "        output = fHt[0] * vn\n",
    "        for n in range(1, nmax):\n",
    "            w = numpy.asarray(self.H @ vn) - α[n-1] * vn - β[n-1] * vnm1\n",
    "            vnm1 = vn\n",
    "            vn = w / β[n]\n",
    "            output = output + fHt[n] * vn\n",
    "        return output\n",
    "    \n",
    "def expm(A, v, **kwargs):\n",
    "    \"\"\"Apply the Lanczos approximation of the exponential exp(1i*dt*A)\n",
    "    onto the vector or matrix `v`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A         -- A matrix or a LinearOperator() with shape (d,d), or\n",
    "                 a linear function that acts on vectors.\n",
    "    v         -- A vector or a matrix with shapes (d,) or (d,M)\n",
    "    d         -- Dimension of matrix A. Only required when A is\n",
    "                 a function or callable object\n",
    "    order     -- maximum order of the Arnoldi approximation\n",
    "    dt        -- time interval in the exponential above\n",
    "    tol       -- relative tolerance for deciding when to stop\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    newv      -- A vector or a matrix approximating expm(1j*dt*A) @ v\n",
    "    \"\"\"\n",
    "    aux = LanczosExpm(A, d=v.size)\n",
    "    return aux.apply(v, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanczosExpm = expm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the algorithms with randomly generated Hermitian operators and one vector. The test compares two types of algorithms:\n",
    "- Versions of Lanczos, Arnoldi and Chebyshev where the number of iterations is fixed\n",
    "- A single run of the same algorithms, but where it chooses the steps\n",
    "In the simulations there is an overall relative tolerance of $10^{-13}$ set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse.linalg as sla\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def chebyTest1(size, **kw):\n",
    "    factor = 1.0\n",
    "    if ('factor' in kw):\n",
    "        factor = kw['factor']\n",
    "    #\n",
    "    # Create a random Hermitian matrix and a vector\n",
    "    #\n",
    "    A = numpy.random.randn(size,size)\n",
    "    A = A @ A.T\n",
    "    v = numpy.random.randn(size)\n",
    "    v /= la.norm(v)\n",
    "    #\n",
    "    # Numerically exact exponential\n",
    "    #\n",
    "    w1 = la.expm(-1j * A * factor) @ v\n",
    "    #\n",
    "    # Set of orders that we wish to test\n",
    "    #\n",
    "    them = [m for m in range(3,size,4)]\n",
    "    #\n",
    "    # Estimate operator norm\n",
    "    #\n",
    "    Anrm = abs(sla.eigsh(A, k=1, which='LM', return_eigenvectors=0)[0]*factor)\n",
    "    #\n",
    "    # Error estimates for Lanczos\n",
    "    #\n",
    "    rho = Anrm/4.0\n",
    "    est = [abs(12*math.exp(m-rho*rho/m)*(rho/m)**m) for m in them]\n",
    "    #\n",
    "    # Error estimates for Lanczos (2nd, bit worse)\n",
    "    #\n",
    "    rho = Anrm/4.0\n",
    "    est2 = [abs(2*math.exp(m*math.log(Anrm/2)-math.lgamma(m))) for m in them]\n",
    "    #\n",
    "    # Actual Lanczos errors\n",
    "    #\n",
    "    aux = LanczosExpm(A)\n",
    "    err1 = [la.norm(w1 - aux.apply(v,order=m,dt=factor)) for m in them]\n",
    "    #\n",
    "    # Selfregulated Lanczos errors\n",
    "    #\n",
    "    err1b = [la.norm(w1 - lanczosExpm(A, v,dt=factor))] * len(them)\n",
    "    #\n",
    "    # Actual Lanczos errors\n",
    "    #\n",
    "    aux = ArnoldiExpm(A)\n",
    "    err2 = [la.norm(w1 - aux.apply(v,order=m,dt=factor)) for m in them]\n",
    "    #\n",
    "    # Selfregulated Arnoldi errors\n",
    "    #\n",
    "    err2b = [la.norm(w1 - arnoldiExpm(A, v,dt=factor))] * len(them)\n",
    "    #\n",
    "    # Adaptive Arnoldi with fixed order\n",
    "    #\n",
    "    err2c = [la.norm(w1 - arnoldiExpm(A, v, dt=factor, adaptive=True, order=10))] * len(them)\n",
    "    #\n",
    "    # Chebyshev class errors\n",
    "    #\n",
    "    aux = ChebyshevExpm(A)\n",
    "    err3 = [la.norm(w1 - aux.apply(v,dt=factor,order=m)) for m in them]\n",
    "    #\n",
    "    # Chebyshev class errors\n",
    "    #\n",
    "    err3b = [la.norm(w1 - chebyshevExpm(A,v,dt=factor))] * len(them)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('$\\\\Vert{A}\\\\Delta{t}\\\\Vert_2$ ='\n",
    "                 f' {Anrm*factor}')\n",
    "    #ax.plot(them, est, '-.', label='Norm bound')\n",
    "    #ax.plot(them, est2, '-.', label='Norm bound2')\n",
    "    ax.plot(them, err1, '.-', label='Lanczos fixed order')\n",
    "    ax.plot(them, err2, '^-', label='Arnoldi fixed order')\n",
    "    ax.plot(them, err2c, '*-', label='Arnoldi adaptive (10)')\n",
    "    ax.plot(them, err3, 'v-', label='Chebyshev fixed order')\n",
    "    ax.plot(them, err1b, '--', label='Lanczos expm')\n",
    "    ax.plot(them, err2b, ':', label='Arnoldi expm')\n",
    "    ax.plot(them, err3b, '--', label='Chebyshev expm')\n",
    "    ax.plot(them, [1e-13]*len(them), '-.', label='Tolerance')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Algorithm steps')\n",
    "    ax.set_ylabel('Exponential error')\n",
    "    ax.legend()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the norm of the operator is small, it converges quickly. There are accuracy warnings due to the version where we set the maximum number of steps. In those cases the algorithm effectively recognizes that the error is above threshold (that is good!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = chebyTest1(50, factor=1/1000.0)\n",
    "ax.set_ylim([1e-16,1e-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the norm of the operator is much larger (or the time step much longer), the algorithms need many more iterations. There are therefore more warnings and convergence only happens much later. In this case it makes sense to decompose the algorithm into multiplications of the same operator with shorter steps, several times (to be done!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = chebyTest1(150, factor=1/10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.sparse.linalg as sla\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def lanczosTiming(size, chebyshev_order=100, factor=1.0, iterations=100):\n",
    "    #\n",
    "    # Create a random Hermitian matrix and a vector\n",
    "    #\n",
    "    A = numpy.random.randn(size,size)\n",
    "    A = numpy.dot(A, A.T)\n",
    "    v = numpy.random.randn(size)\n",
    "    v = v / la.norm(v)\n",
    "    #\n",
    "    aux = LanczosExpm(A)\n",
    "    t = time.process_time()\n",
    "    for i in range(0,iterations):\n",
    "        aux.apply(v,dt=factor)\n",
    "    t = (time.process_time() - t)/iterations\n",
    "    print('LanczosExpm t=',t,'s / iteration')\n",
    "    #\n",
    "    aux = ArnoldiExpm(A)\n",
    "    t = time.process_time()\n",
    "    for i in range(0,iterations):\n",
    "        aux.apply(v,dt=factor)\n",
    "    t = (time.process_time() - t)/iterations\n",
    "    print('Arnoldi t=',t,'s / iteration')\n",
    "    #\n",
    "    aux = ArnoldiExpm(A)\n",
    "    t = time.process_time()\n",
    "    for i in range(0,iterations):\n",
    "        aux.apply(v,adaptive=True,dt=factor)\n",
    "    t = (time.process_time() - t)/iterations\n",
    "    print('Arnoldi adaptive t=',t,'s / iteration')\n",
    "    #\n",
    "    aux = ChebyshevExpm(A)\n",
    "    t = time.process_time()\n",
    "    for i in range(0,iterations):\n",
    "        aux.apply(v,dt=factor,order=chebyshev_order)\n",
    "    t = (time.process_time() - t)/iterations\n",
    "    print('Chebyshev t=',t,'s / iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanczosTiming(300, chebyshev_order=60, factor=1/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lanczosTiming(300, chebyshev_order=90, factor=1/500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
